\documentclass[a4paper,10pt,twocolumn,english]{article}
%---------------------------------------------------------
\usepackage{subcaption}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{bm}
\usepackage{cite}
\usepackage{amsmath}
%---------------------------------------------------------
\newcommand{\eref}[1]{(\ref{#1})}
%---------------------------------------------------------
\title{\textbf{Better Call LoRA}}

\author{
    \begin{tabular}{c c}
        Robert Trifan & Stefan Popa \\
    \end{tabular}\\[0.5em]
    \footnotesize University of Bucharest
}

\date{\empty}

\begin{document}
\maketitle
%---------------------------------------------------------
\begin{abstract} 
Low-rank adaptation (LoRA) has become a lightweight alternative to full fine-tuning for large language models. In this work, we benchmark vanilla LoRA and four recent LoRA variants: swapped-init LoRA (A = 0, B $\sim \mathcal{U}(-0.01, 0.01)$), LoRA-XS ($A \cdot R \cdot B$ factorisation), LoRA+ ($\eta_A \neq \eta_B$ learning rate scaling) and PiSSA-initialised LoRA ($\text{SVD}(W)$ warm-start) on the TinyLlama-1.1 B backbone and the GLUE SST-2 sentiment-classification task.  All experiments were performed on a single RTX 2070 (8 GB), enforcing strict memory budgets. We report classification accuracy, macro-F1, wall-clock training time and peak GPU memory to highlight the trade-offs each variant offers under resource constrained conditions.
\end{abstract}
%---------------------------------------------------------
\section{Introduction}

Large language models (LLMs) have rapidly become the de facto standard for various natural language processing tasks, ranging from search and dialog to code generation and summarization. Their ubiquity is largely attributed to their capacity to learn from vast amounts of data, but this comes at a cost: the computational resources required for training and fine-tuning these models are substantial. 

Training or fully fine-tuning these models typically implies billions of parameters, weeks of GPU time and significant energy consumption. Such costs are often out of reach for many practitioners, leading to a growing interest in more efficient alternatives.

Low-rank adaptation (LoRA) \cite{hu2021loralowrankadaptationlarge} tackles this challenge by injecting a pair of low-rank matrices into the weights of a pre-trained model and learning only those additional parameters while freezing the original model. Despite the simplicity of the approach, LoRA has matched or even surpassed the performance of full fine-tuning on many tasks, while reducing the memory footprint and training time by orders of magnitude.

Building on top of this idea, a growing literature has emerged, proposing improvements that modify the initialization, learning rates schedules or factorisation structure of the low-rank matrices hoping to further enhance the effectiveness of LoRA. While these extensions are promising, their practical impact remains unclear due to inconsistencies in experimental setups.

This survey offers a comprehensive overview of five LoRA variants, including the original LoRA, and benchmarks them on a sentiment classification task using the TinyLlama-1.1B model \cite{zhang2024tinyllamaopensourcesmalllanguage} and the GLUE SST-2 dataset \cite{wang2019gluemultitaskbenchmarkanalysis}. By standardizing the model size, dataset and evaluation metrics, we aim to provide an apples-to-apples comparison of these methods under realistic resource constraints.


\section{Setup}
\noindent\textbf{Model} Describe TinyLlama-1.1B \cite{zhang2024tinyllamaopensourcesmalllanguage} 

\noindent\textbf{Dataset} Desribe GLUE \cite{wang2019gluemultitaskbenchmarkanalysis}

\noindent\textbf{Training} Describe the prompt used for training.

\noindent\textbf{Evaluation} Describe the evaluation step.

\noindent\textbf{Metrics} Describe the metrics used: accuracy, F1, train time, GPU memory.

%---------------------------------------------------------
\section{Low Rank Adaptation}
\noindent\textbf{LoRA} Describe LoRA \cite{hu2021loralowrankadaptationlarge}.

\noindent\textbf{Impact of initialization dynamics on LoRA} Describe different LoRA \cite{hayou2024impactinitializationlorafinetuning}

\noindent\textbf{LoRA-XS} Describe LoRA-XS \cite{balazy2024loraxslowrankadaptationextremely}.

\noindent\textbf{LoRA+} Describe LoRA+ \cite{hayou2024loraefficientlowrank}

\noindent\textbf{PiSSA} Describe PiSSA \cite{meng2025pissaprincipalsingularvalues}

%---------------------------------------------------------
\section{Experiments}
For each method, describe the hyperparameters explored with a table.

Gather the best results into a final table, comparing the methods.

%---------------------------------------------------------
\section{Conclusion}
Explain that, because of limited compute resources, we couldn't see meaningful results.

\bibliography{mybib}
\bibliographystyle{plain}

\end{document}
